{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Knowledge Base System\n",
    "\n",
    "This notebook combines five Python scripts into a complete system for:\n",
    "1. Processing a corpus file\n",
    "2. Vectorizing text chunks\n",
    "3. Creating and querying a vector database\n",
    "4. Running inference with a language model\n",
    "5. A complete knowledge base Q&A system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus Processing\n",
    "\n",
    "This cell reads and splits the corpus file into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.语料库.py\n",
    "with open('语料库.txt', 'r', encoding='utf-8') as fp:\n",
    "    data = fp.read()\n",
    "\n",
    "chunk_list = data.split(\"\\n\\n\")\n",
    "chunk_list = [chunk for chunk in chunk_list if chunk]\n",
    "print(f\"Found {len(chunk_list)} chunks in the corpus.\")\n",
    "print(\"First chunk:\", chunk_list[0] if chunk_list else \"No chunks found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Vectorization\n",
    "\n",
    "This cell demonstrates how to convert text chunks into embeddings using Ollama's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.向量化.py\n",
    "import requests\n",
    "import functools\n",
    "\n",
    "def file_chunk_list():\n",
    "    with open('语料库.txt', 'r', encoding='utf-8') as fp:\n",
    "        data = fp.read()\n",
    "\n",
    "    chunk_list = data.split('\\n\\n')\n",
    "    clist = [chunk for chunk in chunk_list if chunk]\n",
    "    return clist\n",
    "\n",
    "def ollama_api(text):\n",
    "    res = requests.post(\n",
    "        url=\"http://127.0.0.1:11434/api/embeddings\",\n",
    "        json={\n",
    "            \"model\":\"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        }\n",
    "    )\n",
    "    embedding = res.json()['embedding']\n",
    "    return embedding\n",
    "\n",
    "def run_vectorization():\n",
    "    clist = file_chunk_list()\n",
    "    if not clist:\n",
    "        print(\"No chunks found in corpus.\")\n",
    "        return\n",
    "    \n",
    "    # Just process the first chunk as an example\n",
    "    chunk = clist[0]\n",
    "    print(\"Processing chunk:\", chunk[:50] + \"...\" if len(chunk) > 50 else chunk)\n",
    "    vector = ollama_api(chunk)\n",
    "    print(f\"Vector length: {len(vector)}\")\n",
    "    print(f\"First 5 vector values: {vector[:5]}\")\n",
    "\n",
    "run_vectorization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Database\n",
    "\n",
    "This cell demonstrates creating and querying a ChromaDB vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.向量数据库.py\n",
    "import chromadb\n",
    "import uuid\n",
    "import requests\n",
    "\n",
    "def ollama_api(text):\n",
    "    res = requests.post(\n",
    "        url=\"http://127.0.0.1:11434/api/embeddings\",\n",
    "        json={\n",
    "            \"model\":\"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        }\n",
    "    )\n",
    "    embedding = res.json()['embedding']\n",
    "    return embedding\n",
    "\n",
    "def run_vector_database():\n",
    "    # Initialize ChromaDB\n",
    "    client = chromadb.PersistentClient(path=\"db/chroma_demo\")\n",
    "    collection = client.get_or_create_collection(\"collection_v1\")\n",
    "\n",
    "    # Sample documents\n",
    "    documents = [\"线性回归\", \"随机森林\", \"卷积神经网络 (CNN)\", \"支持向量机 (SVM)\", \"K-近邻 (KNN)\", \"梯度提升树 (GBDT)\"]\n",
    "    ids = [str(uuid.uuid4()) for _ in documents]\n",
    "    embeddings = [ollama_api(text) for text in documents]\n",
    "\n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=documents,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "    # Query example\n",
    "    qs = \"随机森林\"\n",
    "    qs_embeddings = ollama_api(qs)\n",
    "\n",
    "    res = collection.query(query_embeddings=[qs_embeddings, ], query_texts=qs, n_results=2)\n",
    "    print(\"Query results:\", res)\n",
    "\n",
    "run_vector_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference Model\n",
    "\n",
    "This cell demonstrates using Ollama's generate API for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.推理模型.py\n",
    "import requests\n",
    "\n",
    "def run_inference():\n",
    "    print(\"输入问题（示例：'解释一下机器学习的基本概念'）：\")\n",
    "    prompt = input()\n",
    "\n",
    "    response = requests.post(\n",
    "        url=\"http://127.0.0.1:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\":\"deepseek-r1:1.5b\",\n",
    "            \"prompt\":prompt,\n",
    "            \"stream\":False\n",
    "        }\n",
    "    )\n",
    "\n",
    "    res = response.json()['response']\n",
    "    print(\"模型回答:\", res)\n",
    "\n",
    "# Uncomment to run\n",
    "# run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Knowledge Base System\n",
    "\n",
    "This cell combines all components into a complete Q&A system with vector database retrieval and LLM generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.知识库.py\n",
    "import chromadb\n",
    "import uuid\n",
    "import requests\n",
    "\n",
    "def file_chunk_list():\n",
    "    with open('语料库.txt', 'r', encoding='utf-8') as fp:\n",
    "        data = fp.read()\n",
    "\n",
    "    chunk_list = data.split('\\n\\n')\n",
    "    clist = [chunk for chunk in chunk_list if chunk]\n",
    "    return clist\n",
    "\n",
    "def ollama_api(text):\n",
    "    res = requests.post(\n",
    "        url=\"http://127.0.0.1:11434/api/embeddings\",\n",
    "        json={\n",
    "            \"model\":\"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        }\n",
    "    )\n",
    "    embedding = res.json()['embedding']\n",
    "    return embedding\n",
    "\n",
    "def ollama_generate_api(prompt):\n",
    "    response = requests.post(\n",
    "        url=\"http://127.0.0.1:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": \"deepseek-llm:7b\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    res = response.json()['response']\n",
    "    return res\n",
    "\n",
    "def initialize_knowledge_base():\n",
    "    client = chromadb.PersistentClient(path=\"db/chroma_demo\")\n",
    "\n",
    "    # Delete existing collection if needed\n",
    "    try:\n",
    "        client.delete_collection(\"collection_v2\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    collection = client.get_or_create_collection(\"collection_v2\")\n",
    "\n",
    "    # Sample documents (in a real system, you'd use your corpus chunks)\n",
    "    documents = [\"线性回归\", \"随机森林\", \"卷积神经网络 (CNN)\", \"支持向量机 (SVM)\", \"K-近邻 (KNN)\", \"梯度提升树 (GBDT)\"]\n",
    "    ids = [str(uuid.uuid4()) for _ in range(len(documents))]\n",
    "    embeddings = [ollama_api(text) for text in documents]\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=documents,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    print(\"Knowledge base initialized with\", len(documents), \"documents.\")\n",
    "\n",
    "def run_knowledge_base():\n",
    "    initialize_knowledge_base()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n输入问题（输入q退出）：\")\n",
    "        qs = input().strip()\n",
    "\n",
    "        if qs.lower() == 'q':\n",
    "            print(\"退出问答系统。\")\n",
    "            break\n",
    "\n",
    "        if not qs:\n",
    "            print(\"请输入有效问题。\")\n",
    "            continue\n",
    "\n",
    "        qs_embedding = ollama_api(qs)\n",
    "\n",
    "        client = chromadb.PersistentClient(path=\"db/chroma_demo\")\n",
    "        collection = client.get_or_create_collection(\"collection_v2\")\n",
    "        res = collection.query(query_embeddings=[qs_embedding, ], query_texts=qs, n_results=2)\n",
    "        result = res[\"documents\"][0]\n",
    "        context = \"\\n\".join(result)\n",
    "        prompt = f\"\"\"你是一个机器学习知识问答机器人，任务是只根据参考信息回答用户问题，不需要除了我给的参考信息之外的信息辅助，如果我给的参考信息没有这方面的知识，请回复\"不知道\"，不要去杜撰任何信息，不要回答不在参考信息外的答案，所有回答请用中文回答\n",
    "        参考信息:{context}，来回答问题:{qs}\"\"\"\n",
    "\n",
    "        answer = ollama_generate_api(prompt)\n",
    "        print(\"\\n回答:\", answer)\n",
    "\n",
    "# Uncomment to run the complete system\n",
    "# run_knowledge_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "1. Run the cells in order to initialize all components\n",
    "2. The last cell contains the complete knowledge base system - uncomment `run_knowledge_base()` to use it\n",
    "3. Make sure you have:\n",
    "   - Ollama running locally\n",
    "   - The required models downloaded (`nomic-embed-text` and `deepseek-llm:7b`)\n",
    "   - A `语料库.txt` file in the same directory\n",
    "   - ChromaDB installed (`pip install chromadb`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
